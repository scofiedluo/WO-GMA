{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## loss test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from train import train_epoch\n",
    "# oarLoss = train_epoch.OARLoss\n",
    "def OARLoss(proposals, per_frame_scores, start_point):\n",
    "    \"\"\"\n",
    "    Onlion action recognizer loss = frame loss + start loss\n",
    "\n",
    "    @param\n",
    "    proposals: N*T*num_class, proposals of TPG, note that if all num_class are zero at a time point, then at this point, \n",
    "    per_frame_scores: N*T*(num_class+1)\n",
    "    start_point: shape: N*T*2\n",
    "    \"\"\"\n",
    "    # append the backgraound lable in proposals\n",
    "    tmp,_ = torch.max(proposals,dim=2,keepdim=True)   # shape N*T*1\n",
    "    inv_tmp = tmp\n",
    "    inv_tmp = torch.ones_like(tmp,dtype=float) - inv_tmp\n",
    "    proposals = torch.cat([proposals,inv_tmp],dim=2)  # N*T*(num_class+1)\n",
    "\n",
    "    # frame_loss\n",
    "    total_T = proposals.shape[0] * proposals.shape[1]\n",
    "    frame_loss = -torch.sum(proposals*torch.log(per_frame_scores))/total_T\n",
    "\n",
    "    # start loss\n",
    "    start_loss = 0.\n",
    "    head = torch.zeros(proposals.shape[0],1,1)\n",
    "    tmp = torch.cat([head,tmp],dim=1)   # shape N*(T+1)*1\n",
    "        \n",
    "    start_label = torch.zeros(proposals.shape[0],proposals.shape[1],2)\n",
    "    for b in range(proposals.shape[0]):\n",
    "        for t in range(1,proposals.shape[1]):\n",
    "            # if previous time block has not action, \n",
    "            # and current time block has action, then current time block is action start point\n",
    "            if tmp[b][t-1][0] == 0 and tmp[b][t][0]==1:\n",
    "                start_label[b][t][0] = 1.\n",
    "            else:\n",
    "                start_label[b][t][1] = 1.\n",
    "    start_loss = -torch.sum(start_label*torch.log(start_point))/total_T\n",
    "    print(start_loss)\n",
    "    return frame_loss+start_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.6012)\n",
      "tensor(1.5199, dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "proposals = torch.Tensor(np.array([[[0,0,0],[1,0,1],[0,0,1],[1,1,1],[0,1,0]],[[0,0,0],[1,0,1],[0,0,1],[1,1,1],[0,1,0]]]))\n",
    "per_frame_scores = torch.rand(2,5,4)\n",
    "start_point = torch.rand(2,5,2)\n",
    "\n",
    "tmp,_ = torch.max(proposals,dim=2,keepdim=True)   # shape N*T*1\n",
    "inv_tmp = tmp\n",
    "inv_tmp = torch.ones_like(tmp) - inv_tmp\n",
    "loss = OARLoss(proposals,per_frame_scores,start_point)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MILoss(topk_score,labels,binary=True,device='cuda'):\n",
    "    \"\"\"\n",
    "    topk_score has the same shape with labels, (N,num_class), where N means batch size\n",
    "    for dataset has one action class only, sigmoid should replace log_softmax\n",
    "    \"\"\"\n",
    "    if binary:\n",
    "        milloss = torch.mean(torch.sum(labels * torch.sigmoid(topk_score), dim=1), dim=0)\n",
    "    else:\n",
    "        milloss = -torch.mean(torch.sum(labels * F.log_softmax(topk_score,dim=1), dim=1), dim=0) # topk.shape = (N,1)\n",
    "    return milloss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def OARLoss(proposals, per_frame_scores, OAR_out):\n",
    "    \"\"\"\n",
    "    Onlion action recognizer loss = frame loss + start loss\n",
    "\n",
    "    @param\n",
    "    proposals: N*T*num_class, proposals of TPG, note that if all num_class are zero at a time point, then at this point, \n",
    "    per_frame_scores: N*T*(num_class+1)\n",
    "    OAR_out: output of LSTM in OAR, shape: N*T*hidden_size\n",
    "    \"\"\"\n",
    "    # append the backgraound lable in proposals\n",
    "    tmp = torch.max(proposals,dim=2,keepdim=True)\n",
    "    tmp = 1 - tmp\n",
    "    proposals = torch.cat([proposals,tmp],dim=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[1., 1.],\n",
      "         [1., 0.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[0., 1.],\n",
      "         [1., 1.],\n",
      "         [1., 1.]]])\n",
      "tensor([[[1.],\n",
      "         [1.],\n",
      "         [0.]],\n",
      "\n",
      "        [[1.],\n",
      "         [1.],\n",
      "         [1.]]])\n",
      "tensor([[[1., 1., 0.],\n",
      "         [1., 0., 0.],\n",
      "         [0., 0., 1.]],\n",
      "\n",
      "        [[0., 1., 0.],\n",
      "         [1., 1., 0.],\n",
      "         [1., 1., 0.]]])\n",
      "tensor(1.2130)\n",
      "tensor([[[0.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [0.]],\n",
      "\n",
      "        [[0.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.]]])\n"
     ]
    }
   ],
   "source": [
    "proposals = torch.Tensor(np.array([[[1,1],[1,0],[0,0]],[[0,1],[1,1],[1,1]]]))\n",
    "print(proposals)\n",
    "tmp,_ = torch.max(proposals,dim=2,keepdim=True)\n",
    "print(tmp)\n",
    "tmp1 = tmp\n",
    "tmp = 1 - tmp\n",
    "proposals = torch.cat([proposals,tmp],dim=2)\n",
    "print(proposals)\n",
    "per_frame_scores = torch.rand(2,3,3)\n",
    "total_T = proposals.shape[0] * proposals.shape[1]\n",
    "frame_loss = -torch.sum(proposals*torch.log(per_frame_scores))/total_T\n",
    "print(frame_loss)\n",
    "\n",
    "head = torch.zeros(proposals.shape[0],1,1)\n",
    "tmp1 = torch.cat([head,tmp1],dim=1)\n",
    "print(tmp1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OAR(nn.Module):\n",
    "    \"\"\"\n",
    "    online action recognizer, \n",
    "    ! For this module, if for real word inference, the input should be x,state = N*C*T,(h_0,c_0),\n",
    "    ! where N = 1 and T = 1, otherwise, N = batch size, T is the num of time block\n",
    "    Input: x,state = N*C*T,(h_0,c_0), where h_0 and c_0 of shape (num_layers * num_directions, batch, hidden_size)\n",
    "    Output: output = N*T*hidden_size, per_frame_socres = N*T*num_class, \n",
    "    and state = (hn, cn), where h_n and c_n of shape (num_layers * num_directions, batch, hidden_size)\n",
    "    here we have num_layers * num_directions = 1\n",
    "\n",
    "    param input_size(int): The number of expected features in the input x\n",
    "    param hidden_size(int): The number of features in the hidden state h\n",
    "    param num_class(int): action classes num and a background class\n",
    "    parma num_layers(int): Number of recurrent layers\n",
    "    \"\"\"\n",
    "    def __init__(self,input_size, hidden_size, num_class, \n",
    "                num_layers=1,M=5, dropout=False, batch_first=True):\n",
    "        super().__init__()\n",
    "        self.M = M\n",
    "        self.rnn = nn.LSTM(input_size,hidden_size,num_layers,dropout,batch_first)\n",
    "        self.linear_scores = nn.Linear(hidden_size,num_class)\n",
    "        self.linear_start = nn.Linear(hidden_size,2)  # TODO: may append layers here\n",
    "        self.softmax = nn.Softmax(dim=2)\n",
    "        self.maxpooling = nn.MaxPool2d(kernel_size=(M,1),stride=1)\n",
    "    \n",
    "\n",
    "    def forward(self,x,state):\n",
    "        # x: N*C*T\n",
    "#         x = x.permute(0,2,1).contiguous()\n",
    "        output, (hn, cn) = self.rnn(x,state)\n",
    "        state = (hn, cn)\n",
    "        output_maxp = output\n",
    "        T1 = output[:,0:1,:]\n",
    "        for i in range(self.M-1):\n",
    "            output_maxp = torch.cat([T1,output_maxp],dim=1)\n",
    "        \n",
    "        start_scores = self.softmax(self.linear_start(self.maxpooling(output_maxp)))\n",
    "        \n",
    "        per_frame_socres = self.softmax(self.linear_scores(output)) # per-frame action scores over classes and background class, shape N*T*num_class\n",
    "        return start_scores\n",
    "oar = OAR(5,6,3)\n",
    "x = torch.randn(2,10,5)\n",
    "start_label = torch.rand(2,10,2)\n",
    "state = (torch.randn(1,2,6),torch.randn(1,2,6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 6, 2])\n",
      "torch.Size([2, 10, 3])\n",
      "start_label= tensor([[[0.9605, 0.6000],\n",
      "         [0.3181, 0.0513],\n",
      "         [0.6731, 0.9493],\n",
      "         [0.7099, 0.0670],\n",
      "         [0.5650, 0.4180],\n",
      "         [0.0447, 0.6420]],\n",
      "\n",
      "        [[0.5821, 0.2074],\n",
      "         [0.5140, 0.3628],\n",
      "         [0.3612, 0.2710],\n",
      "         [0.9077, 0.3066],\n",
      "         [0.3751, 0.5477],\n",
      "         [0.7255, 0.9451]]])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "start_scores,state,per_frame_scores = oar(x,state)\n",
    "print(start_scores.shape)\n",
    "print(per_frame_scores.shape)\n",
    "start_label = torch.rand(2,6,2)\n",
    "print(\"start_label=\",start_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0.5400, 0.4600],\n",
      "         [0.5402, 0.4598],\n",
      "         [0.5378, 0.4622],\n",
      "         [0.5385, 0.4615],\n",
      "         [0.5428, 0.4572],\n",
      "         [0.5443, 0.4557],\n",
      "         [0.5500, 0.4500],\n",
      "         [0.5560, 0.4440],\n",
      "         [0.5564, 0.4436],\n",
      "         [0.5564, 0.4436]],\n",
      "\n",
      "        [[0.6196, 0.3804],\n",
      "         [0.5841, 0.4159],\n",
      "         [0.5718, 0.4282],\n",
      "         [0.5549, 0.4451],\n",
      "         [0.5586, 0.4414],\n",
      "         [0.5588, 0.4412],\n",
      "         [0.5468, 0.4532],\n",
      "         [0.5427, 0.4573],\n",
      "         [0.5414, 0.4586],\n",
      "         [0.5388, 0.4612]]], grad_fn=<SoftmaxBackward>)\n",
      "tensor(15.4635, grad_fn=<NegBackward>)\n",
      "tensor(15.4635, grad_fn=<NegBackward>)\n",
      "tensor([[[0.5389, 0.4611],\n",
      "         [0.5389, 0.4611],\n",
      "         [0.5366, 0.4634],\n",
      "         [0.5373, 0.4627],\n",
      "         [0.5417, 0.4583],\n",
      "         [0.5432, 0.4568],\n",
      "         [0.5492, 0.4508],\n",
      "         [0.5549, 0.4451],\n",
      "         [0.5554, 0.4446],\n",
      "         [0.5554, 0.4446]],\n",
      "\n",
      "        [[0.6181, 0.3819],\n",
      "         [0.5829, 0.4171],\n",
      "         [0.5709, 0.4291],\n",
      "         [0.5539, 0.4461],\n",
      "         [0.5574, 0.4426],\n",
      "         [0.5577, 0.4423],\n",
      "         [0.5461, 0.4539],\n",
      "         [0.5421, 0.4579],\n",
      "         [0.5407, 0.4593],\n",
      "         [0.5383, 0.4617]]], grad_fn=<SoftmaxBackward>)\n",
      "tensor(15.4552, grad_fn=<NegBackward>)\n",
      "tensor(15.4552, grad_fn=<NegBackward>)\n",
      "tensor([[[0.5379, 0.4621],\n",
      "         [0.5376, 0.4624],\n",
      "         [0.5354, 0.4646],\n",
      "         [0.5361, 0.4639],\n",
      "         [0.5405, 0.4595],\n",
      "         [0.5421, 0.4579],\n",
      "         [0.5484, 0.4516],\n",
      "         [0.5539, 0.4461],\n",
      "         [0.5544, 0.4456],\n",
      "         [0.5544, 0.4456]],\n",
      "\n",
      "        [[0.6166, 0.3834],\n",
      "         [0.5817, 0.4183],\n",
      "         [0.5699, 0.4301],\n",
      "         [0.5529, 0.4471],\n",
      "         [0.5563, 0.4437],\n",
      "         [0.5566, 0.4434],\n",
      "         [0.5453, 0.4547],\n",
      "         [0.5415, 0.4585],\n",
      "         [0.5401, 0.4599],\n",
      "         [0.5377, 0.4623]]], grad_fn=<SoftmaxBackward>)\n",
      "tensor(15.4470, grad_fn=<NegBackward>)\n",
      "tensor(15.4470, grad_fn=<NegBackward>)\n",
      "tensor([[[0.5368, 0.4632],\n",
      "         [0.5363, 0.4637],\n",
      "         [0.5342, 0.4658],\n",
      "         [0.5349, 0.4651],\n",
      "         [0.5393, 0.4607],\n",
      "         [0.5410, 0.4590],\n",
      "         [0.5476, 0.4524],\n",
      "         [0.5529, 0.4471],\n",
      "         [0.5534, 0.4466],\n",
      "         [0.5534, 0.4466]],\n",
      "\n",
      "        [[0.6150, 0.3850],\n",
      "         [0.5805, 0.4195],\n",
      "         [0.5689, 0.4311],\n",
      "         [0.5519, 0.4481],\n",
      "         [0.5551, 0.4449],\n",
      "         [0.5555, 0.4445],\n",
      "         [0.5445, 0.4555],\n",
      "         [0.5409, 0.4591],\n",
      "         [0.5394, 0.4606],\n",
      "         [0.5372, 0.4628]]], grad_fn=<SoftmaxBackward>)\n",
      "tensor(15.4388, grad_fn=<NegBackward>)\n",
      "tensor(15.4388, grad_fn=<NegBackward>)\n",
      "tensor([[[0.5357, 0.4643],\n",
      "         [0.5350, 0.4650],\n",
      "         [0.5330, 0.4670],\n",
      "         [0.5337, 0.4663],\n",
      "         [0.5381, 0.4619],\n",
      "         [0.5400, 0.4600],\n",
      "         [0.5468, 0.4532],\n",
      "         [0.5519, 0.4481],\n",
      "         [0.5524, 0.4476],\n",
      "         [0.5524, 0.4476]],\n",
      "\n",
      "        [[0.6135, 0.3865],\n",
      "         [0.5792, 0.4208],\n",
      "         [0.5679, 0.4321],\n",
      "         [0.5509, 0.4491],\n",
      "         [0.5540, 0.4460],\n",
      "         [0.5544, 0.4456],\n",
      "         [0.5438, 0.4562],\n",
      "         [0.5403, 0.4597],\n",
      "         [0.5388, 0.4612],\n",
      "         [0.5366, 0.4634]]], grad_fn=<SoftmaxBackward>)\n",
      "tensor(15.4308, grad_fn=<NegBackward>)\n",
      "tensor(15.4308, grad_fn=<NegBackward>)\n",
      "tensor([[[0.5347, 0.4653],\n",
      "         [0.5337, 0.4663],\n",
      "         [0.5318, 0.4682],\n",
      "         [0.5325, 0.4675],\n",
      "         [0.5369, 0.4631],\n",
      "         [0.5389, 0.4611],\n",
      "         [0.5460, 0.4540],\n",
      "         [0.5509, 0.4491],\n",
      "         [0.5514, 0.4486],\n",
      "         [0.5514, 0.4486]],\n",
      "\n",
      "        [[0.6120, 0.3880],\n",
      "         [0.5780, 0.4220],\n",
      "         [0.5668, 0.4332],\n",
      "         [0.5499, 0.4501],\n",
      "         [0.5529, 0.4471],\n",
      "         [0.5533, 0.4467],\n",
      "         [0.5430, 0.4570],\n",
      "         [0.5397, 0.4603],\n",
      "         [0.5382, 0.4618],\n",
      "         [0.5361, 0.4639]]], grad_fn=<SoftmaxBackward>)\n",
      "tensor(15.4228, grad_fn=<NegBackward>)\n",
      "tensor(15.4228, grad_fn=<NegBackward>)\n",
      "tensor([[[0.5336, 0.4664],\n",
      "         [0.5324, 0.4676],\n",
      "         [0.5306, 0.4694],\n",
      "         [0.5314, 0.4686],\n",
      "         [0.5357, 0.4643],\n",
      "         [0.5378, 0.4622],\n",
      "         [0.5452, 0.4548],\n",
      "         [0.5499, 0.4501],\n",
      "         [0.5504, 0.4496],\n",
      "         [0.5504, 0.4496]],\n",
      "\n",
      "        [[0.6105, 0.3895],\n",
      "         [0.5768, 0.4232],\n",
      "         [0.5658, 0.4342],\n",
      "         [0.5489, 0.4511],\n",
      "         [0.5517, 0.4483],\n",
      "         [0.5523, 0.4477],\n",
      "         [0.5422, 0.4578],\n",
      "         [0.5391, 0.4609],\n",
      "         [0.5375, 0.4625],\n",
      "         [0.5355, 0.4645]]], grad_fn=<SoftmaxBackward>)\n",
      "tensor(15.4150, grad_fn=<NegBackward>)\n",
      "tensor(15.4150, grad_fn=<NegBackward>)\n",
      "tensor([[[0.5325, 0.4675],\n",
      "         [0.5311, 0.4689],\n",
      "         [0.5294, 0.4706],\n",
      "         [0.5302, 0.4698],\n",
      "         [0.5346, 0.4654],\n",
      "         [0.5367, 0.4633],\n",
      "         [0.5444, 0.4556],\n",
      "         [0.5490, 0.4510],\n",
      "         [0.5494, 0.4506],\n",
      "         [0.5494, 0.4506]],\n",
      "\n",
      "        [[0.6089, 0.3911],\n",
      "         [0.5755, 0.4245],\n",
      "         [0.5648, 0.4352],\n",
      "         [0.5479, 0.4521],\n",
      "         [0.5506, 0.4494],\n",
      "         [0.5512, 0.4488],\n",
      "         [0.5415, 0.4585],\n",
      "         [0.5385, 0.4615],\n",
      "         [0.5369, 0.4631],\n",
      "         [0.5350, 0.4650]]], grad_fn=<SoftmaxBackward>)\n",
      "tensor(15.4073, grad_fn=<NegBackward>)\n",
      "tensor(15.4073, grad_fn=<NegBackward>)\n",
      "tensor([[[0.5315, 0.4685],\n",
      "         [0.5298, 0.4702],\n",
      "         [0.5282, 0.4718],\n",
      "         [0.5290, 0.4710],\n",
      "         [0.5334, 0.4666],\n",
      "         [0.5357, 0.4643],\n",
      "         [0.5436, 0.4564],\n",
      "         [0.5480, 0.4520],\n",
      "         [0.5484, 0.4516],\n",
      "         [0.5484, 0.4516]],\n",
      "\n",
      "        [[0.6074, 0.3926],\n",
      "         [0.5743, 0.4257],\n",
      "         [0.5638, 0.4362],\n",
      "         [0.5469, 0.4531],\n",
      "         [0.5495, 0.4505],\n",
      "         [0.5501, 0.4499],\n",
      "         [0.5407, 0.4593],\n",
      "         [0.5379, 0.4621],\n",
      "         [0.5363, 0.4637],\n",
      "         [0.5344, 0.4656]]], grad_fn=<SoftmaxBackward>)\n",
      "tensor(15.3997, grad_fn=<NegBackward>)\n",
      "tensor(15.3997, grad_fn=<NegBackward>)\n",
      "tensor([[[0.5304, 0.4696],\n",
      "         [0.5286, 0.4714],\n",
      "         [0.5270, 0.4730],\n",
      "         [0.5278, 0.4722],\n",
      "         [0.5322, 0.4678],\n",
      "         [0.5346, 0.4654],\n",
      "         [0.5428, 0.4572],\n",
      "         [0.5470, 0.4530],\n",
      "         [0.5475, 0.4525],\n",
      "         [0.5475, 0.4525]],\n",
      "\n",
      "        [[0.6059, 0.3941],\n",
      "         [0.5731, 0.4269],\n",
      "         [0.5628, 0.4372],\n",
      "         [0.5459, 0.4541],\n",
      "         [0.5484, 0.4516],\n",
      "         [0.5490, 0.4510],\n",
      "         [0.5400, 0.4600],\n",
      "         [0.5373, 0.4627],\n",
      "         [0.5356, 0.4644],\n",
      "         [0.5339, 0.4661]]], grad_fn=<SoftmaxBackward>)\n",
      "tensor(15.3922, grad_fn=<NegBackward>)\n",
      "tensor(15.3922, grad_fn=<NegBackward>)\n"
     ]
    }
   ],
   "source": [
    "gd = torch.optim.Adam(oar.parameters())\n",
    "loss_fn = nn.MSELoss()\n",
    "for i in range(10):\n",
    "    start_scores= oar(x,state)\n",
    "    print(start_scores)\n",
    "#     loss = loss_fn(start_scores,start_label)\n",
    "    loss = -torch.sum(start_label*torch.log(start_scores))\n",
    "    print(loss)\n",
    "    #清空前面的导数缓存\n",
    "    gd.zero_grad()\n",
    "    loss.backward(retain_graph=True)\n",
    "    gd.step()\n",
    "    print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = torch.rand(3,5,3)\n",
    "video_level_score = torch.rand(3,3)\n",
    "labels = torch.Tensor(np.array([[1,1,0],[0,1,1],[1,0,0]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.9457)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MILoss(video_level_score,labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CASLoss(features, scores, labels, num_similar,device='cpu'):\n",
    "    \"\"\"\n",
    "    Co-Activity Similarity loss\n",
    "    # TODO: this loss may be refactored to data with different time length in one batch\n",
    "\n",
    "    @param\n",
    "    \"\"\"\n",
    "    attention = F.softmax(scores,dim=1)         #N*T*num_class\n",
    "    features_T = torch.transpose(features,1,2)  #N*f*T\n",
    "    print(\"attention.shape=\",attention.shape)\n",
    "    print(\"features_T.shape=\",features_T.shape)\n",
    "    agg_act = torch.bmm(features_T,attention)    # N*f*num_class\n",
    "    agg_back = torch.bmm(features_T,(1-attention)/attention.shape[1])    #N\n",
    "\n",
    "    co_loss = 0.\n",
    "    n_tmp = 0.       # num of classes in all pair\n",
    "    for i in range(0,num_similar*2,2):      # num_similar*2 <= T\n",
    "        Hf1 = agg_act[i]\n",
    "        Hf2 = agg_act[i+1]\n",
    "        Lf1 = agg_back[i]\n",
    "        Lf2 = agg_back[i+1]\n",
    "\n",
    "        d1 = 1 - torch.sum(Hf1*Hf2, dim=0) / (torch.norm(Hf1, 2, dim=0) * torch.norm(Hf2, 2, dim=0))\n",
    "        d2 = 1 - torch.sum(Hf1*Lf2, dim=0) / (torch.norm(Hf1, 2, dim=0) * torch.norm(Lf2, 2, dim=0))\n",
    "        d3 = 1 - torch.sum(Hf2*Lf1, dim=0) / (torch.norm(Hf2, 2, dim=0) * torch.norm(Lf1, 2, dim=0))\n",
    "        co_loss = co_loss + 0.5*torch.sum(torch.max(d1-d2+0.5, torch.FloatTensor([0.]).to(device))*labels[i,:]*labels[i+1,:])\n",
    "        co_loss = co_loss + 0.5*torch.sum(torch.max(d1-d3+0.5, torch.FloatTensor([0.]).to(device))*labels[i,:]*labels[i+1,:])\n",
    "        n_tmp = n_tmp + torch.sum(labels[i,:]*labels[i+1,:])\n",
    "    co_loss = co_loss / n_tmp\n",
    "    print(n_tmp)\n",
    "    return co_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = torch.randn(3,5,10)\n",
    "scores = torch.rand(3,5,2)\n",
    "# labels = np.array([[[0,1],[1,1],[0,0],[0,1],[1,0]],[[1,1],[0,0],[0,1],[1,0],[1,1]],[[0,0],[0,1],[0,1],[1,0],[0,1]]])\n",
    "labels = np.array([[1,1],[1,1],[0,1]])\n",
    "labels = torch.Tensor(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "features= tensor([[[-1.0516e+00,  4.6353e-01,  7.2436e-01, -8.3697e-01, -4.6175e-01,\n",
      "           5.2051e-01,  1.2845e-01,  8.2604e-01, -4.9437e-01, -4.2670e-01],\n",
      "         [ 1.6683e+00,  2.7589e-02,  9.5760e-01, -5.2163e-01,  1.7666e-01,\n",
      "          -1.0541e+00,  6.0088e-02, -2.9701e+00, -2.0242e-01,  1.4610e-01],\n",
      "         [-6.6417e-01, -2.2485e+00,  6.0095e-01,  1.3202e+00, -6.0029e-01,\n",
      "          -2.3096e-01,  6.8689e-01, -1.3064e+00, -1.1482e+00,  6.0157e-01],\n",
      "         [ 4.7008e-01, -5.5981e-02,  6.8805e-02,  4.2870e-01, -8.1903e-01,\n",
      "           1.7120e+00,  1.7539e+00,  6.6528e-03,  1.0895e+00,  3.8090e-02],\n",
      "         [-4.7366e-01, -1.0655e+00,  5.4189e-01, -1.3752e-01, -1.9368e-01,\n",
      "          -8.2747e-02,  4.8464e-01,  1.9683e-01, -1.4842e-01,  3.4106e-01]],\n",
      "\n",
      "        [[-1.2701e+00, -1.0254e+00, -4.4687e-01, -2.2656e-01,  2.8984e-01,\n",
      "           2.8234e-01, -2.1107e+00, -1.0679e-04, -3.1907e-01,  7.8770e-01],\n",
      "         [ 3.5461e-01,  6.5225e-01,  4.0951e-01,  4.3840e-01,  3.4791e-01,\n",
      "          -1.0205e-01,  1.6108e+00, -4.6863e-01,  1.0402e+00,  7.4672e-01],\n",
      "         [ 8.2786e-02, -8.8595e-01,  2.9993e-01,  5.2732e-01, -6.1087e-01,\n",
      "           6.1353e-01, -2.1457e-01, -5.4405e-01,  4.6986e-01, -7.7748e-01],\n",
      "         [ 2.5171e-02,  1.0898e+00,  4.3358e-01,  1.6139e+00, -6.7027e-01,\n",
      "          -5.2562e-01,  9.7457e-01,  9.8755e-01, -2.4552e-01, -2.3760e-01],\n",
      "         [-1.2654e+00, -4.9261e-01, -5.0184e-01,  2.8133e+00, -1.8495e+00,\n",
      "           1.3212e+00,  1.1300e+00, -5.4576e-01, -1.4032e+00,  1.3361e-01]],\n",
      "\n",
      "        [[-4.9315e-01,  9.3184e-01,  1.9893e-01,  1.3774e-01, -1.5988e+00,\n",
      "           1.7644e+00, -5.1981e-01,  7.6736e-01, -9.9816e-01, -4.8396e-01],\n",
      "         [-5.9620e-01, -5.8942e-01,  1.3024e+00,  8.3900e-01,  5.7388e-01,\n",
      "          -4.6030e-01,  4.2412e-01,  7.6603e-02,  7.8163e-01, -1.4076e+00],\n",
      "         [ 1.4191e-01, -8.8893e-01, -2.4229e-02, -5.5550e-02,  6.3300e-01,\n",
      "          -1.5606e-01,  8.0760e-01,  8.7914e-01,  5.6097e-02, -1.7954e+00],\n",
      "         [-1.4997e+00,  2.0045e+00,  2.7031e+00, -1.8370e+00,  1.2629e-01,\n",
      "           1.2898e+00,  8.4693e-01,  2.0882e+00,  5.1986e-01,  1.4431e-01],\n",
      "         [-1.9202e-01, -4.5780e-02,  1.2903e+00, -2.7523e+00, -1.1337e+00,\n",
      "          -5.3612e-01,  1.2633e+00, -1.0813e+00, -2.1519e+00,  1.0132e+00]]])\n",
      "scores= tensor([[[0.5775, 0.3755],\n",
      "         [0.6192, 0.1362],\n",
      "         [0.1877, 0.7018],\n",
      "         [0.7039, 0.5490],\n",
      "         [0.4778, 0.4940]],\n",
      "\n",
      "        [[0.3578, 0.5679],\n",
      "         [0.3162, 0.2609],\n",
      "         [0.8612, 0.7029],\n",
      "         [0.4274, 0.5222],\n",
      "         [0.7013, 0.3527]],\n",
      "\n",
      "        [[0.5135, 0.3344],\n",
      "         [0.5658, 0.1213],\n",
      "         [0.5615, 0.3561],\n",
      "         [0.7903, 0.4350],\n",
      "         [0.1385, 0.9189]]])\n",
      "labels= tensor([[1., 1.],\n",
      "        [1., 1.],\n",
      "        [0., 1.]])\n"
     ]
    }
   ],
   "source": [
    "print(\"features=\",features)\n",
    "print(\"scores=\",scores)\n",
    "print(\"labels=\",labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attention.shape= torch.Size([3, 5, 2])\n",
      "features_T.shape= torch.Size([3, 10, 5])\n",
      "tensor(2.)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(0.4720)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = CASLoss(features,scores,labels,1)\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attention.shape= torch.Size([3, 5, 2])\n",
      "features_T.shape= torch.Size([3, 10, 5])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.5725,  0.6799],\n",
       "         [-0.3605, -0.3147],\n",
       "         [ 0.1487,  0.1362],\n",
       "         [ 0.2501,  0.2930],\n",
       "         [ 0.1865, -0.0453],\n",
       "         [ 0.7697,  0.7118],\n",
       "         [-0.2322, -0.3537],\n",
       "         [-0.5252, -0.5390],\n",
       "         [-0.8519, -0.9587],\n",
       "         [ 0.3319,  0.3037]],\n",
       "\n",
       "        [[-0.0715,  0.1059],\n",
       "         [ 0.0504,  0.0684],\n",
       "         [ 1.0954,  0.9466],\n",
       "         [ 0.6079,  0.6809],\n",
       "         [-0.0187, -0.0391],\n",
       "         [ 0.0647, -0.1531],\n",
       "         [ 0.6927,  0.6361],\n",
       "         [-0.2934, -0.0114],\n",
       "         [-0.1829, -0.2319],\n",
       "         [ 0.0480,  0.0493]],\n",
       "\n",
       "        [[ 0.0166,  0.0081],\n",
       "         [-0.4705, -0.5331],\n",
       "         [ 0.0952,  0.1041],\n",
       "         [-0.0042,  0.0399],\n",
       "         [-0.8270, -0.5941],\n",
       "         [ 0.0385,  0.2045],\n",
       "         [ 0.2506,  0.1382],\n",
       "         [-0.5096, -0.2524],\n",
       "         [ 0.3008,  0.2737],\n",
       "         [ 0.2848,  0.4801]]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention = F.softmax(scores,dim=1)         #N*T*num_class\n",
    "features_T = torch.transpose(features,1,2)  #N*f*T\n",
    "print(\"attention.shape=\",attention.shape)\n",
    "print(\"features_T.shape=\",features_T.shape)\n",
    "agg_act = torch.bmm(features_T,attention)    # N*f*num_class\n",
    "agg_act"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.8086, 0.2444, 0.7037],\n",
      "        [0.1214, 0.8210, 0.1594],\n",
      "        [0.4395, 0.6299, 0.2168],\n",
      "        [0.8906, 0.0974, 0.1860],\n",
      "        [0.2650, 0.7115, 0.3104]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ True, False,  True],\n",
       "        [False,  True, False],\n",
       "        [False,  True, False],\n",
       "        [ True, False, False],\n",
       "        [False,  True, False]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.rand(5,3)\n",
    "print(x)\n",
    "mask = x.ge(0.5)\n",
    "mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.5525, 0.5471, 0.0743],\n",
       "         [0.9454, 0.7712, 0.3943],\n",
       "         [0.9130, 0.3346, 0.7402],\n",
       "         [0.5996, 0.8005, 0.2182]],\n",
       "\n",
       "        [[0.7448, 0.6191, 0.1369],\n",
       "         [0.9466, 0.0271, 0.8626],\n",
       "         [0.8952, 0.5816, 0.0261],\n",
       "         [0.8032, 0.3378, 0.0813]],\n",
       "\n",
       "        [[0.5613, 0.7013, 0.7648],\n",
       "         [0.4102, 0.3060, 0.1338],\n",
       "         [0.1767, 0.0102, 0.2986],\n",
       "         [0.7390, 0.2893, 0.8552]],\n",
       "\n",
       "        [[0.9061, 0.7105, 0.9145],\n",
       "         [0.4061, 0.1375, 0.2803],\n",
       "         [0.1462, 0.0365, 0.6566],\n",
       "         [0.3384, 0.1836, 0.8128]],\n",
       "\n",
       "        [[0.0223, 0.4694, 0.7929],\n",
       "         [0.7790, 0.1117, 0.5057],\n",
       "         [0.6244, 0.7990, 0.4335],\n",
       "         [0.8096, 0.0447, 0.1649]]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask.unsqueeze(1).shape\n",
    "scores = torch.rand(5,4,3)\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ True, False, False],\n",
       "         [ True, False, False],\n",
       "         [ True, False,  True],\n",
       "         [ True, False, False]],\n",
       "\n",
       "        [[False,  True, False],\n",
       "         [False, False, False],\n",
       "         [False,  True, False],\n",
       "         [False, False, False]],\n",
       "\n",
       "        [[False,  True, False],\n",
       "         [False, False, False],\n",
       "         [False, False, False],\n",
       "         [False, False, False]],\n",
       "\n",
       "        [[ True, False, False],\n",
       "         [False, False, False],\n",
       "         [False, False, False],\n",
       "         [False, False, False]],\n",
       "\n",
       "        [[False, False, False],\n",
       "         [False, False, False],\n",
       "         [False,  True, False],\n",
       "         [False, False, False]]])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask.unsqueeze(1)*scores\n",
    "mask.unsqueeze(1)*scores.ge(0.5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MS_G3D",
   "language": "python",
   "name": "ms_g3d"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
